{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phzg4X8lCkEU"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6_kfjAzkKtV",
        "outputId": "8f2087f4-6a6b-40ae-994a-b1a58752fa89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "%pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lq78WrUJcbT_"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfflI9awCgz3"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZHjxVN3BRljV"
      },
      "outputs": [],
      "source": [
        "# Census Income dataset\n",
        "census_income_kdd = fetch_ucirepo(id=117)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_c = census_income_kdd.data.features\n",
        "y_c = census_income_kdd.data.targets\n",
        "\n",
        "df_c = pd.concat([X_c, y_c], axis=1).reset_index(drop=True)\n",
        "\n",
        "# Create a stratified sample of 40000 rows based on race distribution\n",
        "X_sampled, _, y_sampled, _ = train_test_split(\n",
        "    df_c.drop('ARACE', axis=1),\n",
        "    df_c['ARACE'],\n",
        "    train_size=40000,\n",
        "    stratify=df_c['ARACE'],\n",
        "    random_state=212\n",
        ")\n",
        "\n",
        "# Combine the sampled features and target back into a DataFrame\n",
        "df_c = pd.concat([X_sampled, y_sampled], axis=1).reset_index(drop = True)\n",
        "# df_c = df_c.drop([\"HHDFMX\", ], axis = 1)\n",
        "df_c_m = df_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "ZZCfdq-guwOo",
        "outputId": "5ba3e86c-13fb-4e65-c3b2-ab69c3d8a5ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ARACE\n",
              "White                          33553\n",
              "Black                           4093\n",
              "Asian or Pacific Islander       1170\n",
              "Other                            733\n",
              "Amer Indian Aleut or Eskimo      451\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ARACE</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>White</th>\n",
              "      <td>33553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Black</th>\n",
              "      <td>4093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asian or Pacific Islander</th>\n",
              "      <td>1170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Other</th>\n",
              "      <td>733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amer Indian Aleut or Eskimo</th>\n",
              "      <td>451</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_c[\"ARACE\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td_60iYJtPZB",
        "outputId": "05bb513c-be54-4596-c5b4-569bffa2d34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_c_train shape: (26800, 1070)\n",
            "X_c_test shape: (13200, 1070)\n",
            "y_c_train shape: (26800,)\n",
            "y_c_test shape: (13200,)\n",
            "----------------------------------\n",
            "X_c_m_train shape: (26800, 1070)\n",
            "X_c_m_test shape: (13200, 1070)\n",
            "y_c_m_train shape: (26800,)\n",
            "y_c_m_test shape: (13200,)\n"
          ]
        }
      ],
      "source": [
        "# List of numerical and categorical columns based on the provided dataset\n",
        "numerical_cols = ['AAGE', 'CAPGAIN', 'GAPLOSS', 'MARSUPWRT']\n",
        "categorical_cols = ['ACLSWKR', 'ADTINK', 'ADTOCC', 'AHGA', 'AHSCOL', 'AMARITL', 'AMJIND',\n",
        "                    'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT',\n",
        "                    'DIVVAL', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1']\n",
        "categorical_cols_m = ['ACLSWKR', 'ADTINK', 'ADTOCC', 'AHGA', 'AHSCOL', 'AMJIND',\n",
        "                    'AMJOCC', 'ARACE', 'AREORGN', 'ASEX', 'AUNMEM', 'AUNTYPE', 'AWKSTAT',\n",
        "                    'DIVVAL', 'FILESTAT', 'GRINREG', 'GRINST', 'HHDFMX', 'HHDREL', 'MIGMTR1']\n",
        "\n",
        "\n",
        "# For numerical columns, fill with mean\n",
        "df_c[numerical_cols] = df_c[numerical_cols].fillna(df_c[numerical_cols].mean())\n",
        "for col in numerical_cols:\n",
        "    df_c[col] = df_c[col].fillna(df_c[col].mean())\n",
        "    df_c_m[col] = df_c_m[col].fillna(df_c_m[col].mean())\n",
        "# For categorical columns, fill with mode\n",
        "for col in categorical_cols:\n",
        "    df_c[col] = df_c[col].fillna(df_c[col].mode()[0])\n",
        "for col in categorical_cols_m:\n",
        "    df_c_m[col] = df_c_m[col].fillna(df_c_m[col].mode()[0])\n",
        "\n",
        "le = LabelEncoder()\n",
        "income_class_mapping = dict()\n",
        "race_class_mapping = dict()\n",
        "marital_class_mapping = dict()\n",
        "for column in df_c.columns:\n",
        "    if df_c[column].dtype == object:\n",
        "        if column == \"ARACE\":\n",
        "          df_c[column] = le.fit_transform(df_c[column])\n",
        "          race_class_mapping = {class_label: index for index, class_label in enumerate(le.classes_)}\n",
        "        if(column == \"income\"):\n",
        "          df_c[column] = le.fit_transform(df_c[column])\n",
        "          income_class_mapping = {class_label: index for index, class_label in enumerate(le.classes_)}\n",
        "for column in df_c_m.columns:\n",
        "    if df_c_m[column].dtype == object:\n",
        "        if(column == \"income\"):\n",
        "          df_c_m[column] = le.fit_transform(df_c_m[column])\n",
        "          income_class_mapping = {class_label: index for index, class_label in enumerate(le.classes_)}\n",
        "        if(column == \"AMARITL\"):\n",
        "          df_c_m[column] = le.fit_transform(df_c_m[column])\n",
        "          marital_class_mapping = {class_label: index for index, class_label in enumerate(le.classes_)}\n",
        "\n",
        "\n",
        "# 2. Standardize numerical columns\n",
        "scaler = StandardScaler()\n",
        "df_c_numerical_scaled = scaler.fit_transform(df_c[numerical_cols])\n",
        "df_c_numerical_scaled = pd.DataFrame(df_c_numerical_scaled, columns=numerical_cols)\n",
        "df_c_m_numerical_scaled = scaler.fit_transform(df_c_m[numerical_cols])\n",
        "df_c_m_numerical_scaled = pd.DataFrame(df_c_m_numerical_scaled, columns=numerical_cols)\n",
        "\n",
        "# 3. One-Hot Encode categorical columns\n",
        "# Encode categorical variables\n",
        "ohe = OneHotEncoder()\n",
        "df_c_categorical_encoded = pd.DataFrame(ohe.fit_transform(df_c[categorical_cols]).toarray())\n",
        "df_c_m_categorical_encoded = pd.DataFrame(ohe.fit_transform(df_c_m[categorical_cols]).toarray())\n",
        "\n",
        "\n",
        "# 4. Concatenate the processed numerical and categorical features\n",
        "df_c = pd.concat([df_c[[\"income\", \"ARACE\"]], df_c_numerical_scaled.reset_index(drop=True),\n",
        "                          df_c_categorical_encoded.reset_index(drop=True)], axis=1)\n",
        "df_c_m = pd.concat([df_c_m[[\"income\", \"AMARITL\"]], df_c_m_numerical_scaled.reset_index(drop=True),\n",
        "                          df_c_m_categorical_encoded.reset_index(drop=True)], axis=1)\n",
        "\n",
        "df_c.columns = df_c.columns.astype(str)\n",
        "df_c_m.columns = df_c_m.columns.astype(str)\n",
        "\n",
        "# Separate features and target again\n",
        "X_c = df_c.drop('income', axis=1)\n",
        "y_c = df_c['income']\n",
        "X_c_m = df_c_m.drop('income', axis=1)\n",
        "y_c_m = df_c_m['income']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_c_train, X_c_test, y_c_train, y_c_test = train_test_split(X_c, y_c, test_size=0.33, random_state=212)\n",
        "X_c_m_train, X_c_m_test, y_c_m_train, y_c_m_test = train_test_split(X_c_m, y_c_m, test_size=0.33, random_state=212)\n",
        "\n",
        "print(\"X_c_train shape:\", X_c_train.shape)\n",
        "print(\"X_c_test shape:\", X_c_test.shape)\n",
        "print(\"y_c_train shape:\", y_c_train.shape)\n",
        "print(\"y_c_test shape:\", y_c_test.shape)\n",
        "print(\"----------------------------------\")\n",
        "print(\"X_c_m_train shape:\", X_c_m_train.shape)\n",
        "print(\"X_c_m_test shape:\", X_c_m_test.shape)\n",
        "print(\"y_c_m_train shape:\", y_c_m_train.shape)\n",
        "print(\"y_c_m_test shape:\", y_c_m_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbMLJqljrv6l"
      },
      "source": [
        "## KNN with Custom Distance Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u535njWBn-ZC"
      },
      "outputs": [],
      "source": [
        "def custom_distance(x1, x2, X_train, means, mads, numeric_columns, ordinal_columns):\n",
        "    distance = 0\n",
        "    for i in range(len(x1)):\n",
        "        column_name = X_train.columns[i]\n",
        "\n",
        "        # Interval-scaled features (numeric)\n",
        "        if column_name in numeric_columns:\n",
        "            mean = means[column_name]\n",
        "            mad = mads[column_name]\n",
        "\n",
        "            if mad == 0:\n",
        "                z1 = z2 = 0\n",
        "            else:\n",
        "                z1 = (x1[i] - mean) / mad\n",
        "                z2 = (x2[i] - mean) / mad\n",
        "\n",
        "            distance += abs(z1 - z2)\n",
        "\n",
        "        # Ordinal features (numeric but ordered)\n",
        "        elif column_name in ordinal_columns:\n",
        "            min_value = X_train[column_name].min()\n",
        "            max_value = X_train[column_name].max()\n",
        "\n",
        "            if max_value == min_value:\n",
        "                z1 = z2 = 0\n",
        "            else:\n",
        "                z1 = (x1[i] - min_value) / (max_value - min_value)\n",
        "                z2 = (x2[i] - min_value) / (max_value - min_value)\n",
        "\n",
        "            distance += abs(z1 - z2)\n",
        "\n",
        "        # Nominal (categorical/string) features\n",
        "        else:\n",
        "            if x1[i] == x2[i]:\n",
        "                distance += 0\n",
        "            else:\n",
        "                distance += 1\n",
        "\n",
        "    return distance\n",
        "\n",
        "# Wrapper function for scikit-learn's KNN to work with custom distances\n",
        "class CustomKNN:\n",
        "    def __init__(self, k=16):\n",
        "        self.k = k\n",
        "        self.knn = NearestNeighbors(n_neighbors=k, metric=self.custom_metric)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        \"\"\"Store the training data\"\"\"\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "        # Calculate means and MADs for numeric columns\n",
        "        self.numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
        "        self.ordinal_columns = ['satisfaction_level']  # Example of ordinal columns\n",
        "        self.means = X_train[self.numeric_columns].mean()\n",
        "        self.mads = X_train[self.numeric_columns].apply(lambda x: (x - x.mean()).abs().mean())\n",
        "\n",
        "        # Fit the KNN model using the training data\n",
        "        self.knn.fit(X_train, y_train)\n",
        "\n",
        "    def custom_metric(self, x1, x2):\n",
        "        \"\"\"Custom distance metric passed to sklearn's KNN\"\"\"\n",
        "        return custom_distance(x1, x2, self.X_train, self.means, self.mads, self.numeric_columns, self.ordinal_columns)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"Predict using the trained KNN model\"\"\"\n",
        "        return self.knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkO9KfLsFYfH"
      },
      "source": [
        "## Compute Discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ktAoDD2lEjMr"
      },
      "outputs": [],
      "source": [
        "def compute_discrimination(y_pred, X_test, sensitive_attribute, protected_values, negative_labels):\n",
        "    \"\"\"\n",
        "    Computes discrimination in model predictions.\n",
        "\n",
        "    Args:\n",
        "        y_pred: The labels.\n",
        "        X_test: The test dataset features.\n",
        "        sensitive_attribute: The name of the sensitive attribute (e.g., 'race').\n",
        "        protected_values: List of values of the sensitive attribute representing the protected group.\n",
        "        negative_labels: Negative Labels in the dataset\n",
        "\n",
        "    Returns:\n",
        "        The discrimination score (difference in positive label probability).\n",
        "    \"\"\"\n",
        "\n",
        "    # Create masks for protected and unprotected groups based on the sensitive attribute\n",
        "    protected_mask = X_test[sensitive_attribute].isin(protected_values)\n",
        "    unprotected_mask = ~protected_mask\n",
        "\n",
        "    # Ensure neither group is empty\n",
        "    if sum(protected_mask) == 0 or sum(unprotected_mask) == 0:\n",
        "        return 0  # Avoid division by zero if either group is empty\n",
        "\n",
        "    # Calculate the positive rate for both groups\n",
        "    protected_group_positive_rate = 1 - (sum(pd.Series(y_pred[protected_mask]).isin(negative_labels)) / sum(protected_mask))\n",
        "    unprotected_group_positive_rate = 1 - (sum(pd.Series(y_pred[unprotected_mask]).isin(negative_labels)) / sum(unprotected_mask))\n",
        "\n",
        "    # Calculate the discrimination score\n",
        "    discrimination = unprotected_group_positive_rate - protected_group_positive_rate\n",
        "\n",
        "    return discrimination"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_t_discrimination_percentage(R, t, k, sensitive_attr, protected_value, label_attr, positive_label):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of t-discriminated instances in the dataset R.\n",
        "\n",
        "    Parameters:\n",
        "    - R: pandas DataFrame containing the dataset.\n",
        "    - t: Threshold value for diff(r).\n",
        "    - k: Number of nearest neighbors.\n",
        "    - sensitive_attr: Name of the sensitive attribute in R.\n",
        "    - protected_value: Value of the sensitive attribute that identifies the protected group.\n",
        "    - label_attr: Name of the label attribute in R.\n",
        "    - positive_label: Value of the label that represents the positive class (âŠ•).\n",
        "\n",
        "    Returns:\n",
        "    - percentage: The percentage of t-discriminated instances among the protected group.\n",
        "    - num_t_discriminated: Number of t-discriminated instances.\n",
        "    - total_protected: Total number of instances in the protected group.\n",
        "    \"\"\"\n",
        "    # Split R into protected group P(R) and unprotected group U(R)\n",
        "    P_R = R[R[sensitive_attr] == protected_value].reset_index(drop=True)\n",
        "    U_R = R[R[sensitive_attr] != protected_value].reset_index(drop=True)\n",
        "\n",
        "    # Feature columns (exclude sensitive attribute and label)\n",
        "    feature_cols = [col for col in R.columns if col not in [sensitive_attr, label_attr]]\n",
        "\n",
        "    # Initialize variables\n",
        "    num_t_discriminated = 0\n",
        "    total_protected = len(P_R)\n",
        "\n",
        "    # Precompute NearestNeighbors models\n",
        "    if len(P_R) > 1:\n",
        "        nbrs_P = NearestNeighbors(n_neighbors=min(k+1, len(P_R))).fit(P_R[feature_cols])\n",
        "    else:\n",
        "        nbrs_P = None  # Not enough data for neighbors\n",
        "    if len(U_R) >= 1:\n",
        "        nbrs_U = NearestNeighbors(n_neighbors=min(k, len(U_R))).fit(U_R[feature_cols])\n",
        "    else:\n",
        "        nbrs_U = None  # No unprotected instances\n",
        "\n",
        "    # Iterate over each protected instance\n",
        "    for idx, r in P_R.iterrows():\n",
        "        x_r = r[feature_cols].values.reshape(1, -1)\n",
        "        dec_r = r[label_attr]\n",
        "\n",
        "        # Compute p1 (protected neighbors)\n",
        "        if nbrs_P and len(P_R) > 1:\n",
        "            distances_P, indices_P = nbrs_P.kneighbors(x_r)\n",
        "            indices_P = indices_P[0]\n",
        "            # Exclude the instance itself if present\n",
        "            indices_P = indices_P[indices_P != idx]\n",
        "            k_P = min(k, len(indices_P))\n",
        "            if k_P > 0:\n",
        "                neighbor_labels_P = P_R.iloc[indices_P[:k_P]][label_attr].values\n",
        "                p1 = np.sum(neighbor_labels_P == dec_r) / k_P\n",
        "            else:\n",
        "                p1 = 0\n",
        "        else:\n",
        "            p1 = 0\n",
        "\n",
        "        # Compute p2 (unprotected neighbors)\n",
        "        if nbrs_U and len(U_R) >= 1:\n",
        "            distances_U, indices_U = nbrs_U.kneighbors(x_r)\n",
        "            indices_U = indices_U[0]\n",
        "            k_U = min(k, len(indices_U))\n",
        "            neighbor_labels_U = U_R.iloc[indices_U[:k_U]][label_attr].values\n",
        "            p2 = np.sum(neighbor_labels_U == dec_r) / k_U\n",
        "        else:\n",
        "            p2 = 0\n",
        "\n",
        "        diff_r = p1 - p2\n",
        "\n",
        "        # Check t-discrimination conditions\n",
        "        if (dec_r == positive_label) and (diff_r >= t):\n",
        "            num_t_discriminated += 1\n",
        "\n",
        "    # Calculate percentage\n",
        "    if total_protected > 0:\n",
        "        percentage = (num_t_discriminated / total_protected) * 100\n",
        "    else:\n",
        "        percentage = 0\n",
        "\n",
        "    return percentage, num_t_discriminated, total_protected"
      ],
      "metadata": {
        "id": "HcgST5VfXNAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDTjMaKMCnTw"
      },
      "source": [
        "## Discrimination Discovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uc9K8AZVeICT"
      },
      "outputs": [],
      "source": [
        "def DiscoveryN(R, t, k, sensitive_attr, protected_values, label_attr, negative_labels):\n",
        "    \"\"\"\n",
        "    Implements the DiscoveryN method for discrimination discovery.\n",
        "\n",
        "    Parameters:\n",
        "    - R: pandas DataFrame containing the dataset.\n",
        "    - t: Threshold value for diff(r).\n",
        "    - k: Number of nearest neighbors.\n",
        "    - sensitive_attr: Name of the sensitive attribute in R.\n",
        "    - protected_values: List of values of the sensitive attribute that identify the protected group.\n",
        "    - label_attr: Name of the label attribute in R.\n",
        "    - negative_labels: List of values of the label that represents the negative class.\n",
        "\n",
        "    Returns:\n",
        "    - L: DataFrame containing records from the protected group with 'disc' attribute.\n",
        "    \"\"\"\n",
        "    # Split R into protected group P(R) and unprotected group U(R)\n",
        "    P_R = R[R[sensitive_attr].isin(protected_values)].reset_index(drop=True)\n",
        "    U_R = R[~R[sensitive_attr].isin(protected_values)].reset_index(drop=True)\n",
        "\n",
        "    L_records = []\n",
        "\n",
        "    # Feature columns (exclude sensitive attribute, label, and 'disc' if present)\n",
        "    feature_cols = [col for col in R.columns if col not in [sensitive_attr, label_attr, 'disc']]\n",
        "\n",
        "    # Precompute NearestNeighbors models for U(R)\n",
        "    if len(U_R) >= k:\n",
        "        nbrs_U = NearestNeighbors(n_neighbors=k).fit(U_R[feature_cols])\n",
        "    else:\n",
        "        nbrs_U = NearestNeighbors(n_neighbors=len(U_R)).fit(U_R[feature_cols])\n",
        "\n",
        "    # Iterate over each record in P(R)\n",
        "    for idx, r in P_R.iterrows():\n",
        "        # Prepare P(R) excluding r\n",
        "        P_R_excl_r = P_R.drop(idx).reset_index(drop=True)\n",
        "\n",
        "        x_r = r[feature_cols].values.reshape(1, -1)\n",
        "\n",
        "        # Neighbors in P(R)\\{r}\n",
        "        k_P = min(k, len(P_R_excl_r))\n",
        "        if k_P > 0:\n",
        "            nbrs_P = NearestNeighbors(n_neighbors=k_P).fit(P_R_excl_r[feature_cols])\n",
        "            distances_P, indices_P = nbrs_P.kneighbors(x_r)\n",
        "            ksetP_labels = P_R_excl_r.iloc[indices_P[0]][label_attr].values\n",
        "            p1 = np.sum(ksetP_labels == r[label_attr]) / k_P\n",
        "        else:\n",
        "            p1 = 0\n",
        "\n",
        "        # Neighbors in U(R)\n",
        "        k_U = min(k, len(U_R))\n",
        "        if k_U > 0:\n",
        "            distances_U, indices_U = nbrs_U.kneighbors(x_r)\n",
        "            ksetU_labels = U_R.iloc[indices_U[0]][label_attr].values\n",
        "            p2 = np.sum(ksetU_labels == r[label_attr]) / k_U\n",
        "        else:\n",
        "            p2 = 0\n",
        "\n",
        "        diff = p1 - p2\n",
        "\n",
        "        # Determine 'disc' attribute based on conditions\n",
        "        if (r[label_attr] in negative_labels) and diff >= t:\n",
        "            r['disc'] = 'yes'\n",
        "        else:\n",
        "            r['disc'] = 'no'\n",
        "\n",
        "        L_records.append(r)\n",
        "\n",
        "    # Create DataFrame L from records\n",
        "    L = pd.DataFrame(L_records)\n",
        "    return L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCDU1jJG8yMc"
      },
      "source": [
        "### Race as Sensitive Attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mb7Ec839eMgt"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "t = 0.1\n",
        "k = 16\n",
        "sensitive_attr = 'ARACE'\n",
        "protected_values = [race_class_mapping[v] for v in [' Black', ' Amer Indian Aleut or Eskimo',' Asian or Pacific Islander', ' Other']]\n",
        "label_attr = 'income'\n",
        "negative_labels = [income_class_mapping[v] for v in ['-50000']]\n",
        "\n",
        "# Run DiscoveryN\n",
        "L = DiscoveryN(df_c, t, k, sensitive_attr, protected_values, label_attr, negative_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZanHO95x0Vz",
        "outputId": "5650e047-33dc-478b-fc81-1e85bc7669c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9209302325581395\n",
            "Precision: 0.323943661971831\n",
            "Recall: 0.2987012987012987\n",
            "F1-score: 0.3108108108108108\n"
          ]
        }
      ],
      "source": [
        "X_L = L.drop(['disc'], axis=1)\n",
        "y_L = L['disc']\n",
        "\n",
        "\n",
        "# Split L into train and test sets\n",
        "X_L_train, X_L_test, y_L_train, y_L_test = train_test_split(X_L, y_L, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a DecisionTreeClassifier on L\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_L_train, y_L_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_L_pred = clf.predict(X_L_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_L_test, y_L_pred)\n",
        "precision = precision_score(y_L_test, y_L_pred, pos_label='yes')\n",
        "recall = recall_score(y_L_test, y_L_pred, pos_label='yes')\n",
        "f1 = f1_score(y_L_test, y_L_pred, pos_label='yes')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3wXN5Y_83BX"
      },
      "source": [
        "### Marital Status as Sensitive Attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lSZRBOBb8u_M"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "t = 0.1\n",
        "k = 16\n",
        "sensitive_attr = 'AMARITL'\n",
        "protected_values = [marital_class_mapping[v] for v in [' Divorced', ' Separated', ' Widowed']]\n",
        "label_attr = 'income'\n",
        "negative_labels = [income_class_mapping[v] for v in ['-50000']]\n",
        "\n",
        "# Run DiscoveryN\n",
        "L = DiscoveryN(df_c_m, t, k, sensitive_attr, protected_values, label_attr, negative_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eL4dDhqOBRcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2d61da-e387-4ac5-f046-459f9e4d5d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.896358543417367\n",
            "Precision: 0.22058823529411764\n",
            "Recall: 0.2054794520547945\n",
            "F1-score: 0.21276595744680848\n"
          ]
        }
      ],
      "source": [
        "X_L = L.drop(['disc'], axis=1)\n",
        "y_L = L['disc']\n",
        "\n",
        "# Split L into train and test sets\n",
        "X_L_train, X_L_test, y_L_train, y_L_test = train_test_split(X_L, y_L, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a DecisionTreeClassifier on L\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_L_train, y_L_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_L_pred = clf.predict(X_L_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_L_test, y_L_pred)\n",
        "precision = precision_score(y_L_test, y_L_pred, pos_label='yes')\n",
        "recall = recall_score(y_L_test, y_L_pred, pos_label='yes')\n",
        "f1 = f1_score(y_L_test, y_L_pred, pos_label='yes')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fTDcxzkDUkq"
      },
      "source": [
        "## Discrimation Prevention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZmMWLni5eVEU"
      },
      "outputs": [],
      "source": [
        "def PreventionN(T, V, t, k, sensitive_attr, protected_values, label_attr, negative_labels, classifierName = \"DT\"):\n",
        "    \"\"\"\n",
        "    Implements the PreventionN method.\n",
        "\n",
        "    Parameters:\n",
        "    - T: pandas DataFrame representing the training dataset.\n",
        "    - V: pandas DataFrame representing the validation dataset.\n",
        "    - t: Threshold value for diff(r).\n",
        "    - k: Number of nearest neighbors.\n",
        "    - sensitive_attr: Name of the sensitive attribute in T.\n",
        "    - protected_values: Value of the sensitive attribute that identifies the protected group.\n",
        "    - label_attr: Name of the label attribute in T.\n",
        "    - negative_labels: List of the labels that represents the negative class.\n",
        "\n",
        "    Returns:\n",
        "    - classifier_T: Classifier trained on the original training dataset T.\n",
        "    - classifier_T_prime: Classifier trained on the modified training dataset T'.\n",
        "    - performance: A dictionary containing accuracy scores on V for both classifiers.\n",
        "    \"\"\"\n",
        "    # Copy of the training data for T'\n",
        "    T_prime = T.copy()\n",
        "\n",
        "    # Split T into protected and unprotected groups\n",
        "    P_T = T[T[sensitive_attr].isin(protected_values)].reset_index(drop=True)\n",
        "    U_T = T[~T[sensitive_attr].isin(protected_values)].reset_index(drop=True)\n",
        "\n",
        "    # Feature columns (exclude sensitive attribute and label)\n",
        "    feature_cols = [col for col in T.columns if col not in [sensitive_attr, label_attr]]\n",
        "\n",
        "    # Precompute NearestNeighbors models\n",
        "    if len(P_T) > 1:\n",
        "        nbrs_P = NearestNeighbors(n_neighbors=min(k, len(P_T)-1)).fit(P_T[feature_cols])\n",
        "    else:\n",
        "        nbrs_P = None  # Not enough data\n",
        "    if len(U_T) >= k:\n",
        "        nbrs_U = NearestNeighbors(n_neighbors=k).fit(U_T[feature_cols])\n",
        "    else:\n",
        "        nbrs_U = NearestNeighbors(n_neighbors=len(U_T)).fit(U_T[feature_cols])\n",
        "\n",
        "    # Iterate over each record in T\n",
        "    for idx, r in T.iterrows():\n",
        "        # Check if record is protected\n",
        "        is_protected = r[sensitive_attr] in protected_values\n",
        "\n",
        "        x_r = r[feature_cols].values.reshape(1, -1)\n",
        "        dec_r = r[label_attr]\n",
        "\n",
        "        # Compute diff(r) only if necessary\n",
        "        if is_protected:\n",
        "            # Neighbors in P_T excluding r\n",
        "            if nbrs_P and len(P_T) > 1:\n",
        "                P_T_excl_r = P_T.drop(P_T.index[idx if idx < len(P_T) else -1]).reset_index(drop=True)\n",
        "                k_P = min(k, len(P_T_excl_r))\n",
        "                if k_P > 0:\n",
        "                    nbrs_P_excl = NearestNeighbors(n_neighbors=k_P).fit(P_T_excl_r[feature_cols])\n",
        "                    distances_P, indices_P = nbrs_P_excl.kneighbors(x_r)\n",
        "                    ksetP_labels = P_T_excl_r.iloc[indices_P[0]][label_attr].values\n",
        "                    p1 = np.sum(ksetP_labels == dec_r) / k_P\n",
        "                else:\n",
        "                    p1 = 0\n",
        "            else:\n",
        "                p1 = 0\n",
        "\n",
        "            # Neighbors in U_T\n",
        "            k_U = min(k, len(U_T))\n",
        "            if k_U > 0:\n",
        "                distances_U, indices_U = nbrs_U.kneighbors(x_r)\n",
        "                ksetU_labels = U_T.iloc[indices_U[0]][label_attr].values\n",
        "                p2 = np.sum(ksetU_labels == dec_r) / k_U\n",
        "            else:\n",
        "                p2 = 0\n",
        "\n",
        "            diff = p1 - p2\n",
        "        else:\n",
        "            diff = 0  # For unprotected, diff is not computed or used\n",
        "\n",
        "        # Conditions to modify the decision attribute in T_prime\n",
        "        if (dec_r in negative_labels) and is_protected and (diff >= t):\n",
        "            T_prime.at[idx, label_attr] = income_class_mapping[\" 50000+.\"]\n",
        "\n",
        "    # Build classifiers on T and T_prime\n",
        "    X_T = T[feature_cols]\n",
        "    y_T = T[label_attr]\n",
        "\n",
        "    X_T_prime = T_prime[feature_cols]\n",
        "    y_T_prime = T_prime[label_attr]\n",
        "\n",
        "    if(classifierName == \"DT\"):\n",
        "      classifier_T = DecisionTreeClassifier()\n",
        "    elif(classifierName == \"NB\"):\n",
        "      classifier_T = GaussianNB()\n",
        "    elif(classifierName == \"LR\"):\n",
        "      classifier_T = LogisticRegression()\n",
        "    classifier_T.fit(X_T, y_T)\n",
        "\n",
        "    if(classifierName == \"DT\"):\n",
        "      classifier_T_prime = DecisionTreeClassifier()\n",
        "    elif(classifierName == \"NB\"):\n",
        "      classifier_T_prime = GaussianNB()\n",
        "    elif(classifierName == \"LR\"):\n",
        "      classifier_T_prime = LogisticRegression()\n",
        "    classifier_T_prime.fit(X_T_prime, y_T_prime)\n",
        "\n",
        "    # Evaluate classifiers on validation set V\n",
        "    X_V = V[feature_cols]\n",
        "    y_V = V[label_attr]\n",
        "\n",
        "    y_pred_T = classifier_T.predict(X_V)\n",
        "    y_pred_T_prime = classifier_T_prime.predict(X_V)\n",
        "\n",
        "    performance = {\n",
        "        'accuracy_T': accuracy_score(y_V, y_pred_T),\n",
        "        'discrimination_T': compute_discrimination(y_pred_T, pd.concat([X_V, V[[\"race\"]]], axis = 1), sensitive_attr, protected_values, negative_labels),\n",
        "        'accuracy_T_prime': accuracy_score(y_V, y_pred_T_prime),\n",
        "        'discrimination_T_prime': compute_discrimination(y_pred_T_prime, pd.concat([X_V, V[[\"race\"]]], axis = 1), sensitive_attr, protected_values, negative_labels),\n",
        "    }\n",
        "\n",
        "    return classifier_T, classifier_T_prime, performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1sTZUOLHPW"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Race"
      ],
      "metadata": {
        "id": "Kimp1ULDnmRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-PmJSPlniPxw"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "t = 0.1\n",
        "k = 16\n",
        "sensitive_attr = 'ARACE'\n",
        "protected_values = [race_class_mapping[v] for v in [' Black', ' Amer Indian Aleut or Eskimo',' Asian or Pacific Islander', ' Other']]\n",
        "label_attr = 'income'\n",
        "negative_labels = [income_class_mapping[v] for v in ['-50000']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGDNfxBYJyTQ",
        "outputId": "be43abd0-ae09-4a87-ee67-9748a6327417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9253787878787879,\n",
            " 'discrimination_T': 0.040649700831287587,\n",
            " 'accuracy_T_prime': 0.9170454545454545,\n",
            " 'discrimination_T_prime': 0.017234623988881426}\n"
          ]
        }
      ],
      "source": [
        "# 0.1\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_train, y_c_train], axis = 1), pd.concat([X_c_test, y_c_test], axis = 1), t, k, sensitive_attr, protected_values, label_attr, negative_labels, \"DT\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9uVNsCAFByVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23be348-2a9f-4c85-b9a2-875684a9eaa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.88825757575757575,\n",
            " 'discrimination_T': 0.11710605026003873,\n",
            " 'accuracy_T_prime': 0.8790151515151515,\n",
            " 'discrimination_T_prime': 0.09464417153822181}\n"
          ]
        }
      ],
      "source": [
        "# 0.1\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_train, y_c_train], axis = 1), pd.concat([X_c_test, y_c_test], axis = 1), t, k, sensitive_attr, protected_values, label_attr, negative_labels, \"NB\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AcVlFKxDBybd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52820ca1-6eb9-438d-da89-cb5d8d0c871f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9486363636363636,\n",
            " 'discrimination_T': 0.020847829172955912,\n",
            " 'accuracy_T_prime': 0.9346212121212121,\n",
            " 'discrimination_T_prime': -0.017900813063304377}\n"
          ]
        }
      ],
      "source": [
        "# 0.1\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_train, y_c_train], axis = 1), pd.concat([X_c_test, y_c_test], axis = 1), t, k, sensitive_attr, protected_values, label_attr, negative_labels, \"LR\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd87oCsw7v-Y",
        "outputId": "23417f33-dc17-4b59-885e-49c97dfad696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.923030303030303,\n",
            " 'discrimination_T': 0.019734294516318474,\n",
            " 'accuracy_T_prime': 0.9073484848484848,\n",
            " 'discrimination_T_prime': -0.09971345736243775}\n"
          ]
        }
      ],
      "source": [
        "# 0.05\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_train, y_c_train], axis = 1), pd.concat([X_c_test, y_c_test], axis = 1), 0.05, k, sensitive_attr, protected_values, label_attr, negative_labels, \"DT\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eX22gKaQB8zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c236c4-5941-4cd4-e9a0-b50cd3f9a7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.88825757575757575,\n",
            " 'discrimination_T': 0.11710605026003873,\n",
            " 'accuracy_T_prime': 0.834469696969697,\n",
            " 'discrimination_T_prime': 0.075671842531305}\n"
          ]
        }
      ],
      "source": [
        "# 0.05\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_train, y_c_train], axis = 1), pd.concat([X_c_test, y_c_test], axis = 1), 0.05, k, sensitive_attr, protected_values, label_attr, negative_labels, \"NB\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZPezYHTyJV7",
        "outputId": "0ae080d0-af8e-4ebf-fcd9-39827c885d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9486363636363636,\n",
            " 'discrimination_T': 0.020847829172955912,\n",
            " 'accuracy_T_prime': 0.934469696969697,\n",
            " 'discrimination_T_prime': -0.09621653661237639}\n"
          ]
        }
      ],
      "source": [
        "# 0.05\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_train, y_c_train], axis = 1), pd.concat([X_c_test, y_c_test], axis = 1), 0.05, k, sensitive_attr, protected_values, label_attr, negative_labels, \"LR\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Marital Status"
      ],
      "metadata": {
        "id": "iq9-ESGynpgf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "locc3g0TCuMg"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "t = 0.1\n",
        "k = 16\n",
        "sensitive_attr = 'AMARITL'\n",
        "protected_values = [marital_class_mapping[v] for v in [' Divorced', ' Separated', ' Widowed']]\n",
        "label_attr = 'income'\n",
        "negative_labels = [income_class_mapping[v] for v in ['-50000']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_dI5kGG3EFuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85347978-99b5-40b0-bb03-e6544570fed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9246212121212121,\n",
            " 'discrimination_T': 0.0307641889382847822,\n",
            " 'accuracy_T_prime': 0.9113636363636364,\n",
            " 'discrimination_T_prime': 0.01860417648813219}\n"
          ]
        }
      ],
      "source": [
        "# 0.1\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_m_train, y_c_m_train], axis = 1), pd.concat([X_c_m_test, y_c_m_test], axis = 1), t, k, sensitive_attr, protected_values, label_attr, negative_labels, \"DT\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cOLubonnEFxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be8a987-4f7f-4dbf-deba-30f887cf9def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.88825757575757575,\n",
            " 'discrimination_T': -0.3098426872588048,\n",
            " 'accuracy_T_prime': 0.87303030303030305,\n",
            " 'discrimination_T_prime': -0.31603418530204497}\n"
          ]
        }
      ],
      "source": [
        "# 0.1\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_m_train, y_c_m_train], axis = 1), pd.concat([X_c_m_test, y_c_m_test], axis = 1), t, k, sensitive_attr, protected_values, label_attr, negative_labels, \"NB\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QqHGyLSmEGAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483b0178-1e44-42f3-abdc-6aed5a4a3caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9486363636363636,\n",
            " 'discrimination_T': 0.009884360850877183,\n",
            " 'accuracy_T_prime': 0.9470454545454545,\n",
            " 'discrimination_T_prime': -0.026509636628728628}\n"
          ]
        }
      ],
      "source": [
        "# 0.1\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_m_train, y_c_m_train], axis = 1), pd.concat([X_c_m_test, y_c_m_test], axis = 1), t, k, sensitive_attr, protected_values, label_attr, negative_labels, \"LR\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pQ5LOXPoERJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d31af0c-9088-4223-9ecc-60728181962c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9252272727272727,\n",
            " 'discrimination_T': -0.00014435174470028844,\n",
            " 'accuracy_T_prime': 0.906969696969697,\n",
            " 'discrimination_T_prime': -0.15660210927619944}\n"
          ]
        }
      ],
      "source": [
        "# 0.05\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_m_train, y_c_m_train], axis = 1), pd.concat([X_c_m_test, y_c_m_test], axis = 1), 0.05, k, sensitive_attr, protected_values, label_attr, negative_labels, \"DT\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C990Lb-lERRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22fc929-3a52-42cc-f218-8ec9e9ff237f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.8846969696969697,\n",
            " 'discrimination_T': 0.04687500231808417,\n",
            " 'accuracy_T_prime': 0.82727272727272,\n",
            " 'discrimination_T_prime': 0.01625218363539806}\n"
          ]
        }
      ],
      "source": [
        "# 0.05\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_m_train, y_c_m_train], axis = 1), pd.concat([X_c_m_test, y_c_m_test], axis = 1), 0.05, k, sensitive_attr, protected_values, label_attr, negative_labels, \"NB\")\n",
        "performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Mmtzu3QGERel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fb5810-58b2-47e9-d7b9-f91ca54b5b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy_T': 0.9440151515151516,\n",
            " 'discrimination_T': 0.0036914781386230278,\n",
            " 'accuracy_T_prime': 0.9053030303030303,\n",
            " 'discrimination_T_prime': -0.1590719631772991}\n"
          ]
        }
      ],
      "source": [
        "# 0.05\n",
        "classifier_T, classifier_T_prime, performance = PreventionN(pd.concat([X_c_m_train, y_c_m_train], axis = 1), pd.concat([X_c_m_test, y_c_m_test], axis = 1), 0.05, k, sensitive_attr, protected_values, label_attr, negative_labels, \"LR\")\n",
        "performance"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}